---
title:  "开发Spark应用骨架介绍"
categories: [Spark]
tags: [Spark,Spark应用]
date: 2018-11-22 08:56:00
layout: post
published: true
---

经过一段时间开发Spark应用的积累, 在工程实践中摸索出了一些开发Spark套路. 主要分为启动类、工程打包、提交任务、测试、调试，日志, 整合性能组件，对接数据源, cookbook。在接下来的一段时间里，会将这些实践不断得添加到工程里面，最终整理成一个分模块的工程，可以各取所需，快速解决遇到的问题。前期主要总结出一套Java开发Spark应用的项目骨架,帮助初次接触Spark的新手,后面会增加scala开发相关的内容.

# 主要模块介绍
1. 启动类
    封装了Spark常用的一些上下文类, 虽然最简单的例子可以通过`SparkSession.builder().getOrCreate()'可以启动一个最简单的应用, spark-shell也自动创建好了这些对象, 但是这些做法在实际的应用开发环境中只能用来验证简单场景. 这个启动类的目的就是封装一些实际开发需要用的一些参数, 同时透出一些可配置项, 方便日常开发使用.

2. 工程打包

    由于整个工程大部分还是Java开发, 加上Maven目前还是比较广泛使用的Java工程构建, 所以这一块会介绍下怎么打包Spark应用, 方便部署.

3. 测试

    大数据开发一个比较蛋疼的问题就是相比web工程来说, 利用线上实际数据来验证程序的成本会非常高, 这时候单元测试就显得格外重要了, 跑数据验证的时间完全可以用来写代码了,这样还提高了程序的可靠性.这部分会集成一些方便好用的测试工具类,方便快速验证逻辑.

4. 调试

    Spark程序在生产环境run的时候总有异常的情况,由于测试环境的差异性, 有些case线下不一定能还原, 在这个时候能在线调试就很重要了.

5. 日志
    
    日志对于排查线上的异常情况非常重要, 但是由于Spark部署方式的差异性, 日志的使用方式也会有差异, 这部分会介绍日志相关的实践.
    
6. 整合性能组件

    大数据开发中,当数据量到达一定规模, 而机器资源又有限的情况下, 这时候对应用的性能优化显得就很有必要了, 这部分会介绍下和现在主流的一些性能组件的集成, 对于节省机器成本还是有意义的.
    
7. 对接数据源
    在实际的开发中, 需要使用的数据源多种多样, 这部分会重点介绍对接常用的存储以及一些使用上的实践.
    
8. cookbook

   这个部分主要是提供解决特定场景问题的方案, 目的是直接用这段代码就能解决特定问题.
   
# 开发环境
* Java:1.8
* Scala:2.11
* 构建工具:Maven
* Spark:2.3.1
* 版本管理:Git
   
目前脉络还不是太清晰, 这几大块会随着总结的过程中调整,慢慢丰满起来 总结的顺序不固定,最终会将文章目录整理到该页面.
